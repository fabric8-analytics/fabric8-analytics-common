# NOTE: Containers use the generic "coreapi" prefix so they're independent of
#       the particular project name. Project specific images are always built
#       locally when using the docker-compose file.
#
#   The services are all configured with "network_mode: bridge" to work around
#   name lookup problems with docker, docker-compose and the use of a local DNS
#   resolver on the host: https://bugzilla.redhat.com/show_bug.cgi?id=1258350
version: "2"
services:
  broker:
    # if you're updating tag, don't forget to also update configuration
    # for kubernetes in orchestration/restart-coreapi.sh
    image: registry.centos.org/centos/rabbitmq
    container_name: coreapi-broker
    network_mode: bridge
    ports:
     - "5672:5672"
     - "15672:15672"
    environment:
      RABBITMQ_USER: guest
      RABBITMQ_PASS: guest
  postgres:
    image: registry.devshift.net/bayesian/coreapi-postgres
    network_mode: bridge
    ports:
     - "6432:5432"
    environment:
      POSTGRESQL_USER: coreapi
      POSTGRESQL_PASSWORD: coreapi
      POSTGRESQL_DATABASE: "coreapi;anitya"
    container_name: coreapi-postgres
  pgbouncer:
    image: registry.devshift.net/bayesian/coreapi-pgbouncer
    container_name: coreapi-pgbouncer
    network_mode: bridge
    links:
     - postgres
    ports:
     - "5432:5432"
    environment:
      POSTGRESQL_USER: coreapi
      POSTGRESQL_PASSWORD: coreapi
      POSTGRESQL_DATABASE: coreapi
      POSTGRESQL_INITIAL_DATABASE: postgres
  server:
    image: registry.devshift.net/bayesian/bayesian-api
    network_mode: bridge
    links:
     - anitya-server
     - broker
     - pgbouncer
     - gremlin-http
    container_name: coreapi-server
    environment:
      F8A_DEBUG: 'true'
      DEPLOYMENT_PREFIX: "${USER}"
      WORKER_ADMINISTRATION_REGION: api
      # Provide credentials here if you want to run on Amazon SQS instead of RabbitMQ, don't forget to supply
      # credentials even for worker
      #AWS_SQS_ACCESS_KEY_ID: ''
      #AWS_SQS_SECRET_ACCESS_KEY: ''
      #AWS_S3_ACCESS_KEY_ID: ''
      #AWS_S3_SECRET_ACCESS_KEY: ''
      # Both can be omitted, defaults to eu-west-1
      #AWS_SQS_REGION: ''
      #AWS_S3_REGION: ''
      POSTGRESQL_USER: coreapi
      POSTGRESQL_PASSWORD: coreapi
      POSTGRESQL_DATABASE: coreapi
      PGBOUNCER_SERVICE_HOST: coreapi-pgbouncer
      BAYESIAN_GREMLIN_HTTP_SERVICE_HOST: "bayesian-gremlin-http"
      BAYESIAN_GREMLIN_HTTP_SERVICE_PORT: "8182"
      DISABLE_AUTHENTICATION: 1
      # you can change it and/or generate a JWT token in server/hack/auth_test_fixtures
      # BAYESIAN_FETCH_PUBLIC_KEY: 'https://sso.openshift.io/auth/realms/fabric8/'
      # BAYESIAN_JWT_AUDIENCE: 'fabric8-online-platform'
      BAYESIAN_AUTH_KEY: |
        -----BEGIN PUBLIC KEY-----
        MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAs73IBTo0rX2m9bGJGOFq
        NtD2XiN6Y3mLjYDnBILRHVQ3fyZnTy/pRC+aKQl/rFbJbv0cXH/WdqOUGv4o5csV
        caR7CPWPPNJg4RrkgtrJGAY5Zxu0A4SflyUI6RMnxbrleP/3+bHjS5W4xFUtX/uL
        8Um+wlwuR26tPeQAe5HyGNo/mmDNzqohQGVca89qKf/HFnKmYLeMcaWQAH/o0KSK
        yZVEKlG689y3K0Tq6XYBC+SkOIrsWcg71ZrX6azm8DPBa6/hSck619H+ILe7VwjX
        pZ4sS5sLo10E0sSHNZb57o8MpGTPBHQPgKNNnHGlTn2MyzmLPZm8OCr+KuFtmCxO
        MwIDAQAB
        -----END PUBLIC KEY-----
    ports:
     - "32000:5000"
  jobs:
    image: registry.devshift.net/bayesian/coreapi-jobs
    network_mode: bridge
    container_name: coreapi-jobs
    restart: always
    ports:
     - "34000:34000"
    depends_on:
     # forces docker-compose to build the image after its base worker image
     - worker-api
     - worker-ingestion
     - pgbouncer
    links:
     - broker
     - pgbouncer
     - minio-s3
    environment:
      # Uncomment if you want to start job service in a paused state
      #JOB_SERVICE_PAUSED: 1
      #JOB_SERVICE_PORT: 34000
      DEPLOYMENT_PREFIX: "${USER}"
      DISABLE_AUTHENTICATION: "1"
      # We use ingestion in deployment, but force to api here as we have only one worker that is serving api requests by default
      WORKER_ADMINISTRATION_REGION: api
      RABBITMQ_SERVICE_SERVICE_HOST: coreapi-broker
      POSTGRESQL_USER: coreapi
      POSTGRESQL_PASSWORD: coreapi
      POSTGRESQL_DATABASE: coreapi
      PGBOUNCER_SERVICE_HOST: coreapi-pgbouncer
      # Provide credentials here if you want to run on Amazon SQS instead of RabbitMQ, don't forget to supply
      # credentials even for server and worker
      # Both can be omitted, defaults to eu-west-1
      #AWS_SQS_REGION: ''
      #AWS_S3_REGION: ''
  worker-api: &worker
    image: registry.devshift.net/bayesian/cucos-worker
    restart: always
    network_mode: bridge
    depends_on:
     - worker-db-migrations
    links:
     - anitya-server
     - broker
     - pgbouncer
     - minio-s3
     - data-model-importer
     - gremlin-http
    environment: &worker_environment
      DEPLOYMENT_PREFIX: "${USER}"
      WORKER_ADMINISTRATION_REGION: api
      RABBITMQ_SERVICE_SERVICE_HOST: coreapi-broker
      F8A_SERVER_SERVICE_HOST: coreapi-server
      POSTGRESQL_USER: coreapi
      POSTGRESQL_PASSWORD: coreapi
      POSTGRESQL_DATABASE: coreapi
      PGBOUNCER_SERVICE_HOST: coreapi-pgbouncer
      # Sync data to Scality-S3
      BAYESIAN_SYNC_S3: 1
      # 0 - Bayesian runs inside RH
      # 1 - Bayesian runs in a cloud
      OPENSHIFT_DEPLOYMENT: 0
      # Provide credentials here if you want to run on Amazon SQS instead of RabbitMQ, don't forget to supply
      # credentials even for server
      #AWS_SQS_ACCESS_KEY_ID: ''
      #AWS_SQS_SECRET_ACCESS_KEY: ''
      #AWS_S3_ACCESS_KEY_ID: ''
      #AWS_S3_SECRET_ACCESS_KEY: ''
      # Both can be omitted, defaults to eu-west-1
      #AWS_SQS_REGION: ''
      #AWS_S3_REGION: ''
      # If no Github API token is provided, requests will be unauthenticated, i.e. limited to 60 per hour
      # Generate your token @ https://github.com/settings/tokens
      #GITHUB_TOKEN: ""
      #PULP_URL: ""
      #PULP_USERNAME: ""
      #PULP_PASSWORD: ""
      #BLACKDUCK_HOST: ""
      #BLACKDUCK_SCHEME: ""
      #BLACKDUCK_PORT: ""
      #BLACKDUCK_USERNAME: ""
      #BLACKDUCK_PASSWORD: ""
      #BLACKDUCK_PATH: ""
      JACCARD_THRESHOLD: 0.4
      SIMILARITY_SCORE_THRESHOLD: 0.4
      MAX_COMPANION_PACKAGES: 5
      MAX_ALTERNATE_PACKAGES: 2
      OUTLIER_THRESHOLD: 0.6
      UNKNOWN_PACKAGES_THRESHOLD: 0.3
      PGM_SERVICE_HOST: "kronos-stack-analysis.dev.rdu2c.fabric8.io"
      PGM_SERVICE_PORT: "80"
      LICENSE_SERVICE_HOST: "stack-license-stack-license.dev.rdu2c.fabric8.io"
      LICENSE_SERVICE_PORT: "80"
      BAYESIAN_GREMLIN_HTTP_SERVICE_HOST: "bayesian-gremlin-http"
      BAYESIAN_GREMLIN_HTTP_SERVICE_PORT: "8182"
      BAYESIAN_DATA_IMPORTER_SERVICE_HOST: "data-model-importer"
      BAYESIAN_DATA_IMPORTER_SERVICE_PORT: "9192"
      # Provide credentials from Google's key.json here:
      GCP_TYPE: ""
      GCP_PROJECT_ID: ""
      GCP_PRIVATE_KEY_ID: ""
      GCP_PRIVATE_KEY: ""
      GCP_CLIENT_EMAIL: ""
      GCP_CLIENT_ID: ""
      GCP_AUTH_URI: ""
      GCP_TOKEN_URI: ""
      GCP_AUTH_PROVIDER_X509_CERT_URL: ""
      GCP_CLIENT_X509_CERT_URL: ""
    tty: true  # yes, really -ti -d, binwalk chokes when there's no tty kept open
  worker-ingestion:
    <<: *worker
    environment:
      <<: *worker_environment
      WORKER_ADMINISTRATION_REGION: ingestion
  worker-db-migrations:
    image: registry.devshift.net/bayesian/cucos-worker
    restart: on-failure
    container_name: coreapi-worker-db-migrations
    network_mode: bridge
    links:
     - pgbouncer
    command: /alembic/run-db-migrations.sh
    environment:
      POSTGRESQL_USER: coreapi
      POSTGRESQL_PASSWORD: coreapi
      POSTGRESQL_DATABASE: coreapi
      POSTGRESQL_INITIAL_DATABASE: postgres
      PGBOUNCER_SERVICE_HOST: coreapi-pgbouncer
      WORKER_ADMINISTRATION_REGION: ingestion
      WORKER_RUN_DB_MIGRATIONS: "1"
  anitya-server:
    image: registry.devshift.net/bayesian/anitya-server
    ports:
     - "31005:5000"
    links:
     - postgres
    network_mode: bridge
    container_name: anitya-server
    environment:
      ANITYA_POSTGRES_SERVICE_HOST: coreapi-postgres
      ANITYA_POSTGRES_SERVICE_PORT: 5432
      POSTGRESQL_USER: coreapi
      POSTGRESQL_PASSWORD: coreapi
      POSTGRESQL_DATABASE: anitya
      POSTGRESQL_INITIAL_DATABASE: postgres
  minio-s3:
    image: minio/minio
    command:
        - server
        - --address
        - ":33000"
        - /export
    container_name: coreapi-s3
    network_mode: bridge
    ports:
      - "33000:33000"
    environment:
      MINIO_ACCESS_KEY: GNV3SAHAHA3DOT99GQII
      MINIO_SECRET_KEY: ZmvMwngonaDK5ymlCd6ptaalDdJsCn3aSSxASPaZ

  dynamodb:
    image: hilverd/dynamodb
    network_mode: bridge
    ports:
      - "8000:8000"
      - "4567:4567"
    mem_limit: 2G
    container_name: dynamodb

  gremlin-http:
    image: registry.devshift.net/bayesian/gremlin
    network_mode: bridge
    entrypoint: /bin/entrypoint-local.sh
    environment:
      - REST=1
      - DEBUG_GRAPH_METRICS=0
    ports:
      - "8181:8182"
    depends_on:
      - dynamodb
    mem_limit: 2G
    container_name: bayesian-gremlin-http
    links:
      - dynamodb

  data-model-importer:
    image: registry.devshift.net/bayesian/data-model-importer
    restart: always
    network_mode: bridge
    entrypoint:
      - /bin/entrypoint.sh
    # volumes:
    #   - ./data-model/src:/src:z
    environment:
      DATA_IMPORTER_SERVICE_PORT: "9192"
      DATA_IMPORTER_SERVICE_TIMEOUT: "3600"
      NUMBER_WORKER_PROCESS: "1"
      # controls whether to use http/ws or https/wss
      GREMLIN_USE_SECURE_CONNECTION: "false"
      BAYESIAN_GREMLIN_HTTP_SERVICE_HOST: "bayesian-gremlin-http"
      BAYESIAN_GREMLIN_HTTP_SERVICE_PORT: "8182"
      AWS_S3_IS_LOCAL: 1
      AWS_S3_ACCESS_KEY_ID: "GNV3SAHAHA3DOT99GQII"
      AWS_S3_SECRET_ACCESS_KEY: "ZmvMwngonaDK5ymlCd6ptaalDdJsCn3aSSxASPaZ"
      DEPLOYMENT_PREFIX: "${USER}"
      AWS_EPV_BUCKET: "${USER}-bayesian-core-data"
      AWS_PKG_BUCKET: "${USER}-bayesian-core-package-data"
      POSTGRESQL_USER: coreapi
      POSTGRESQL_PASSWORD: coreapi
      POSTGRESQL_DATABASE: coreapi
      PGBOUNCER_SERVICE_HOST: coreapi-pgbouncer
      BAYESIAN_PGBOUNCER_SERVICE_HOST: "coreapi-pgbouncer"
      BAYESIAN_PGBOUNCER_SERVICE_PORT: 5432
    ports:
      - "9192:9192"
    links:
      - gremlin-http
      - minio-s3
  cvedb-s3-dump:
    # this service just copies pre-built CVE DB to S3 and exits
    # https://github.com/fabric8-analytics/cvedb-s3-dump-docker
    image: registry.devshift.net/bayesian/cvedb-s3-dump
    restart: on-failure
    network_mode: bridge
    environment:
      AWS_ACCESS_KEY_ID: "GNV3SAHAHA3DOT99GQII"
      AWS_SECRET_ACCESS_KEY: "ZmvMwngonaDK5ymlCd6ptaalDdJsCn3aSSxASPaZ"
      DEPLOYMENT_PREFIX: "${USER}"
      S3_ENDPOINT_URL: "http://coreapi-s3:33000"
    links:
      - minio-s3
